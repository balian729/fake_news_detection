{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem statement**\n",
    "\n",
    "In this day and age, when people get most of the news from the internet, where the sources cannot always be credible, the spread of fake news became a significant issue. With the exponential growth of social media platforms and online news sources, identifying and mitigating the impact of false information has become a daunting challenge. Traditional methods of fake news detection often fall short in handling the dynamic and evolving nature of deceptive content.\n",
    "\n",
    "## **Solution description**\n",
    "\n",
    "In this context, leveraging Graph Machine Learning (Graph ML) techniques for fake news detection presents a promising avenue. Graph ML enables the representation of relationships and interactions among entities, making it well-suited for capturing the intricate patterns of information propagation and source credibility in online networks.\n",
    "\n",
    "In our project, we tried to develop a model that will be able to detect fake news and for this, we decided to use UPFD[1] dataset and compared the performance of different models on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "\n",
    "The dataset consists of Twitter networks that propagate both fake and real news, constructed based on fact-check information from Politifact[2] and Gossipcop[3]. The retweet graphs for the news were initially obtained using FakeNewsNet[4]. The dataset's statistics are presented below:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Data</strong>\n",
    "   </td>\n",
    "   <td><strong>#Graphs</strong>\n",
    "   </td>\n",
    "   <td><strong>#Fake News</strong>\n",
    "   </td>\n",
    "   <td><strong>#Total Nodes</strong>\n",
    "   </td>\n",
    "   <td><strong>#Total Edges</strong>\n",
    "   </td>\n",
    "   <td><strong>#Avg. Nodes per Graph</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Politifact\n",
    "   </td>\n",
    "   <td>314\n",
    "   </td>\n",
    "   <td>157\n",
    "   </td>\n",
    "   <td>41,054\n",
    "   </td>\n",
    "   <td>40,740\n",
    "   </td>\n",
    "   <td>131\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Gossipcop\n",
    "   </td>\n",
    "   <td>5464\n",
    "   </td>\n",
    "   <td>2732\n",
    "   </td>\n",
    "   <td>314,262\n",
    "   </td>\n",
    "   <td>308,798\n",
    "   </td>\n",
    "   <td>58\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "In our case, we used only Politifact as our dataset, since Gossipcop was too large and required much more computational resources.\n",
    "\n",
    "Please keep in mind that there are **four node feature types** in the dataset. The 768-dimensional BERT and 300-dimensional spaCy features are encoded using pretrained models from BERT[5] and spaCy[6] word2vec, respectively. Additionally, the 10-dimensional profile feature is derived from profiles of  Twitter accounts, and content feature that is a combination of spacy and profile features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is Graph ML and PyG?**\n",
    "\n",
    "So, what is Graph ML? Think of Graph ML as a way to teach computers about relationships and connections. In real-world problems, things are often connected or related to each other, like friends in a social network or molecules in chemistry. Graph ML helps computers understand and make predictions based on these connections. \n",
    "\n",
    "A graph consists of nodes (representing entities) and edges (representing relationships between entities). Each node and edge can have associated attributes. There are many different Graph ML models, and those of them that were used in this project, I am going to explain further down below.\n",
    "\n",
    "And what is PyG? PyG is an acronym for PyTorch Geometric, which is an extension library for PyTorch specifically tailored for dealing with graph-structured data. It provides abstractions for handling graphs, creating graph datasets, and implementing various graph neural network layers.\n",
    "\n",
    "Key features of PyG:\n",
    "\n",
    "* **Data Handling**: PyG provides a `Data` class to manage node and edge attributes, making it easy to work with graph data. \n",
    "* **Graph Convolutional Layers**: PyG includes various graph convolutional layers, such as Graph Convolutional Networks (GCNs), GraphSAGE, etc., simplifying the implementation of GNNs. \n",
    "* **Dataset Loading**: PyG includes datasets for common benchmarks, simplifying the process of loading and working with well-known graph datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, DataParallel\n",
    "from torch_geometric.nn import global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Node feature types**\n",
    "\n",
    "There are node four feature types that were used, each introducing their own set of features based on the selected approach\n",
    "\n",
    "* **bert** - 768-dimensional feature type which is encoded using pretrained BERT\n",
    "* **spacy** - 300-dimensional feature type which is encoded using pretrained spaCy word2vec, respectively\n",
    "* **profile** - 10-dimensional feature type which represents data taken from Twitter accounts. This data include: verified status(binary), geo-spatial positioning(binary), the amount of followers and friends, date of account creation, etc.\n",
    "* **content** - a combination of two other node feature types:  \n",
    "    * 300-dimensional `spacy`, capturing semantic content.\n",
    "    * 10-dimensional `profile`, providing additional user-specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models and techniques**\n",
    "### **GCN**\n",
    "\n",
    "GCN stands for Graph Convolutional Network, and it's a type of neural network architecture designed to work with graph-structured data.\n",
    "\n",
    "#### **Graph Convolution Operation:**\n",
    "**Transformation of Node Features:** \n",
    "  - Each node is associated with a feature vector. In the case of the first layer, these are the initial node features. \n",
    "  - A weight matrix is applied to these features, similar to the weight matrices in traditional neural network layers.\n",
    "\n",
    "$Z = \\sigma(A \\cdot X \\cdot W_1)$\n",
    "\n",
    "where:\n",
    "  - $X$ is the input node features matrix.\n",
    "  - $W_1$ is the weight matrix for the first layer.\n",
    "  - $A$ is the adjacency matrix.\n",
    "  - $\\sigma$ is the activation function.\n",
    "\n",
    "**Aggregation of Neighboring Information:** \n",
    "\n",
    "* The convolutional operation aggregates information from neighboring nodes. \n",
    "* This is achieved by multiplying the adjacency matrix A with the transformed node features Z.\n",
    "\n",
    "$Z' = \\sigma(A \\cdot Z \\cdot W_2)$\n",
    "\n",
    "where:\n",
    " - $W_2$ is the weight matrix for the second layer.\n",
    "\n",
    "**Output:**\n",
    "The result $Z'$ represents the new features of each node after considering information from its neighbors.\n",
    "\n",
    "#### **Training**\n",
    "Training a GCN involves optimizing the weights $W_1$ and $W_2$ to minimize a specific loss function for the task at hand. Commonly used loss functions include cross-entropy for classification tasks or mean squared error for regression tasks.\n",
    "\n",
    "### **GAT**\n",
    "\n",
    "The Graph Attention Network (GAT) is a type of neural network architecture designed for graph-structured data. In the context of GAT, a graph is represented as a set of nodes and edges. Each node in the graph corresponds to an entity, and edges represent relationships or connections between entities.\n",
    "\n",
    "The core idea behind GAT is the use of attention mechanisms. Attention mechanisms allow the model to focus on different parts of the input when making predictions, mimicking the human attention process. GAT employs a self-attention mechanism, where each node can attend to its neighbors with different attention weights. This allows the model to assign different importance to different neighbors during information aggregation.\n",
    "\n",
    "GAT computes a node's representation by aggregating information from its neighbors. The aggregation is done using attention weights, which are learned during training. The node representations are computed as a weighted sum of the neighboring node features, where the attention weights are determined by the compatibility between the node and its neighbors.\n",
    "\n",
    "GAT extends the basic attention mechanism by using multiple attention heads. Each attention head learns a different set of attention weights, capturing different aspects of the relationships between nodes. The outputs of multiple attention heads are concatenated or averaged to produce the final node representations.\n",
    "\n",
    "Given a graph with nodes represented by feature vectors, the GAT operation for computing the output features of a node involves computing attention coefficients, applying them to the corresponding neighbor node features, and aggregating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(torch.nn.Module):\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(SimpleGNN, self).__init__()\n",
    "\t\tself.args = args\n",
    "\t\tself.num_features = args.num_features\n",
    "\t\tself.nhid = args.nhid\n",
    "\t\tself.num_classes = args.num_classes\n",
    "\t\tself.model = args.model\n",
    "\t\tself.concat = args.concat\n",
    "\n",
    "\t\tif self.model == 'gcn':\n",
    "\t\t\tself.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "\t\telif self.model == 'graphsage':\n",
    "\t\t\tself.conv1 = SAGEConv(self.num_features, self.nhid)\n",
    "\t\telif self.model == 'gat':\n",
    "\t\t\tself.conv1 = GATConv(self.num_features, self.nhid)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tself.lin0 = torch.nn.Linear(self.num_features, self.nhid)\n",
    "\t\t\tself.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "\n",
    "\t\tself.lin2 = torch.nn.Linear(self.nhid, self.num_classes)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\n",
    "\t\tx, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "\t\tedge_attr = None\n",
    "\n",
    "\t\tx = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "\t\tx = gmp(x, batch)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tnews = torch.stack([data.x[(data.batch == idx).nonzero().squeeze()[0]] for idx in range(data.num_graphs)])\n",
    "\t\t\tnews = F.relu(self.lin0(news))\n",
    "\t\t\tx = torch.cat([x, news], dim=1)\n",
    "\t\t\tx = F.relu(self.lin1(x))\n",
    "\n",
    "\t\tx = F.log_softmax(self.lin2(x), dim=-1)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GraphSAGE**\n",
    "\n",
    "GraphSAGE, which stands for Graph Sample and Aggregated (SAGE), is a graph neural network (GNN) architecture designed for learning node embeddings in a graph. The main objective of GraphSAGE is to learn a function that generates embeddings for nodes in a graph in an inductive manner. In other words, it aims to produce embeddings for nodes that were not seen during training, enabling the model to generalize to unseen nodes.\n",
    "\n",
    "\n",
    "#### **Graph Convolution**:\n",
    "GraphSAGE utilizes a graph convolutional operation to aggregate information from a node's local neighborhood. Unlike traditional convolutional operations in image processing, graph convolutional operations consider the graph structure.\n",
    "\n",
    "\n",
    "#### **Aggregation Strategy**\n",
    "\n",
    "The key innovation of GraphSAGE lies in its sampling and aggregation strategy. Rather than aggregating information from all neighboring nodes, which could be computationally expensive for large graphs, GraphSAGE samples a fixed-size neighborhood around each node.\n",
    "\n",
    "1. **Sampling**: For each node, a fixed-size sample of its neighbors is chosen. The sampling strategy can be random, but it's often guided by a more structured approach like sampling neighbors from a fixed radius or using a specific sampling algorithm.\n",
    "\n",
    "2. **Aggregation**: The sampled node features are then aggregated to create a representative vector for the node. This aggregation process is typically a differentiable operation, and it allows the model to learn how to combine information from the neighborhood.\n",
    "\n",
    "#### **GraphSAGE Architecture:**\n",
    "\n",
    "The GraphSAGE model typically consists of multiple layers of the sampling and aggregation process. The output of each layer is used as the input for the next layer. The final layer's output represents the node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(torch.nn.Module):\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(SimpleGNN, self).__init__()\n",
    "\t\tself.args = args\n",
    "\t\tself.num_features = args.num_features\n",
    "\t\tself.nhid = args.nhid\n",
    "\t\tself.num_classes = args.num_classes\n",
    "\t\tself.model = args.model\n",
    "\t\tself.concat = args.concat\n",
    "\n",
    "\t\tif self.model == 'gcn':\n",
    "\t\t\tself.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "\t\telif self.model == 'graphsage':\n",
    "\t\t\tself.conv1 = SAGEConv(self.num_features, self.nhid)\n",
    "\t\telif self.model == 'gat':\n",
    "\t\t\tself.conv1 = GATConv(self.num_features, self.nhid)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tself.lin0 = torch.nn.Linear(self.num_features, self.nhid)\n",
    "\t\t\tself.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "\n",
    "\t\tself.lin2 = torch.nn.Linear(self.nhid, self.num_classes)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\n",
    "\t\tx, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "\t\tedge_attr = None\n",
    "\n",
    "\t\tx = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "\t\tx = gmp(x, batch)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tnews = torch.stack([data.x[(data.batch == idx).nonzero().squeeze()[0]] for idx in range(data.num_graphs)])\n",
    "\t\t\tnews = F.relu(self.lin0(news))\n",
    "\t\t\tx = torch.cat([x, news], dim=1)\n",
    "\t\t\tx = F.relu(self.lin1(x))\n",
    "\n",
    "\t\tx = F.log_softmax(self.lin2(x), dim=-1)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GCNFN [7]**\n",
    "GCNFN is implemented using two GCN layers and one mean-pooling layer as the graph encoder; \n",
    "the 310-dimensional node feature (args.feature = content) is composed of 300-dimensional \n",
    "comment word2vec (spaCy) embeddings plus 10-dimensional profile features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNFN(torch.nn.Module):\n",
    "\t\"\"\"\n",
    "GCNFN is implemented using two GCN layers and one mean-pooling layer as the graph encoder; \n",
    "the 310-dimensional node feature (args.feature = content) is composed of 300-dimensional \n",
    "comment word2vec (spaCy) embeddings plus 10-dimensional profile features \n",
    "\n",
    "Paper: Fake News Detection on Social Media using Geometric Deep Learning\n",
    "Link: https://arxiv.org/pdf/1902.06673.pdf\n",
    "\n",
    "\n",
    "Model Configurations:\n",
    "\n",
    "Vanilla GCNFN: args.concat = False, args.feature = content\n",
    "UPFD-GCNFN: args.concat = True, args.feature = spacy\n",
    "\"\"\"\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(GCNFN, self).__init__()\n",
    "\n",
    "\t\tself.num_features = args.num_features\n",
    "\t\tself.num_classes = args.num_classes\n",
    "\t\tself.nhid = args.nhid\n",
    "\t\tself.concat = args.concat\n",
    "\n",
    "\t\tself.conv1 = GATConv(self.num_features, self.nhid * 2)\n",
    "\t\tself.conv2 = GATConv(self.nhid * 2, self.nhid * 2)\n",
    "\n",
    "\t\tself.fc1 = Linear(self.nhid * 2, self.nhid)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tself.fc0 = Linear(self.num_features, self.nhid)\n",
    "\t\t\tself.fc1 = Linear(self.nhid * 2, self.nhid)\n",
    "\n",
    "\t\tself.fc2 = Linear(self.nhid, self.num_classes)\n",
    "\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tx, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "\t\tx = F.selu(self.conv1(x, edge_index))\n",
    "\t\tx = F.selu(self.conv2(x, edge_index))\n",
    "\t\tx = F.selu(global_mean_pool(x, batch))\n",
    "\t\tx = F.selu(self.fc1(x))\n",
    "\t\tx = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "\t\tif self.concat:\n",
    "\t\t\tnews = torch.stack([data.x[(data.batch == idx).nonzero().squeeze()[0]] for idx in range(data.num_graphs)])\n",
    "\t\t\tnews = F.relu(self.fc0(news))\n",
    "\t\t\tx = torch.cat([x, news], dim=1)\n",
    "\t\t\tx = F.relu(self.fc1(x))\n",
    "\n",
    "\t\tx = F.log_softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metrics**\n",
    "To evaluate the performance of our models, we used four different metrics: These metrics being accuracy, AUC, F1 score and loss. Below you can see how each metric is calculated and the reasoning behind its selection.\n",
    "\n",
    "### Accuracy\n",
    "As you know, accuracy is one of the most basic metrics and can be simply calculated as (# of correct predictions)/(total prediction number). The reason behind choosing it as it is the most simple metric that allows any person to clearly understand the performance of our models. It is also worth mentioning that our dataset is fairly balanced, with fake news making up ~40% of the dataset, and real news being ~60%, respectively.\n",
    "### AUC\n",
    "As it is known AUC stands for Area Under ROC Curve, ROC here meaning Receiver Operating Characteristic, which is sometimes called an “error curve”. We decided to chose that metric as it is excellent for binary classification and suits a slight imbalance in our data\n",
    "### F1 score\n",
    "F1 score is one of the most used evaluation metrics as it integrates precision and recall into a single metric to gain a better understanding of model performance. It’s calculated as follows:\n",
    "F1 = (2 * _Precision_ * _Recall_)(_Precision_ + _Recall_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "\taccuracy_score, \n",
    "\trecall_score, \n",
    "\tprecision_score, \n",
    "\troc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\tUtility functions for evaluating the model performance\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def eval_deep(log, loader):\n",
    "\t\"\"\"\n",
    "\tEvaluating the classification performance given mini-batch data\n",
    "\t\"\"\"\n",
    "\n",
    "\t# get the empirical batch_size for each mini-batch\n",
    "\tdata_size = len(loader.dataset.indices)\n",
    "\tbatch_size = loader.batch_size\n",
    "\tif data_size % batch_size == 0:\n",
    "\t\tsize_list = [batch_size] * (data_size//batch_size)\n",
    "\telse:\n",
    "\t\tsize_list = [batch_size] * (data_size // batch_size) + [data_size % batch_size]\n",
    "\n",
    "\tassert len(log) == len(size_list)\n",
    "\n",
    "\taccuracy, f1, precision, recall = 0, 0, 0, 0\n",
    "\n",
    "\tprob_log, label_log = [], []\n",
    "\n",
    "\tfor batch, size in zip(log, size_list):\n",
    "\t\tpred_y, y = batch[0].data.cpu().numpy().argmax(axis=1), batch[1].data.cpu().numpy().tolist()\n",
    "\t\tprob_log.extend(batch[0].data.cpu().numpy()[:, 1].tolist())\n",
    "\t\tlabel_log.extend(y)\n",
    "\n",
    "\t\taccuracy += accuracy_score(y, pred_y) * size\n",
    "\t\tf1 += f1_score(y, pred_y, average='binary') * size\n",
    "\t\tprecision += precision_score(y, pred_y, zero_division=0) * size\n",
    "\t\trecall += recall_score(y, pred_y, zero_division=0) * size\n",
    "\n",
    "\tauc = roc_auc_score(label_log, prob_log)\n",
    "\n",
    "\treturn [accuracy/data_size, f1/data_size, precision/data_size, recall/data_size, auc]\n",
    "\n",
    "def compute_test(model, loader, device=torch.device('cpu'), verbose=False):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_test = 0.0\n",
    "\t\tout_log = []\n",
    "\t\tfor data in loader:\n",
    "\t\t\tdata = data.to(device)\n",
    "\t\t\tout = model(data)\n",
    "\t\t\ty = data.y\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(F.softmax(out, dim=1).cpu().numpy())\n",
    "\t\t\tout_log.append([F.softmax(out, dim=1), y])\n",
    "\t\t\tloss_test += F.nll_loss(out, y).item()\n",
    "\tloss_test /= len(loader)\n",
    "\tres = eval_deep(out_log, loader)\n",
    "\tres.append(loss_test)\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train(model, train_loader, val_loader, args):\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\tmodel.train()\n",
    "\n",
    "\tbest_loss_val = 1e9\n",
    "\tbest_metrics = {\n",
    "\t\t\"acc\": 0,\n",
    "\t\t\"auc\": 0,\n",
    "\t\t\"loss\": 0,\n",
    "\t\t\"F1\": 0\n",
    "\t}\n",
    "\tloop = trange(args.epochs)\n",
    "\n",
    "\n",
    "\tfor _ in loop:\n",
    "\t\tout_log = []\n",
    "\t\tloss_train = 0.0\n",
    "\t\tfor i, data in enumerate(train_loader):\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tdata = data.to(args.device)\n",
    "\t\t\tout = model(data)\n",
    "\t\t\ty = data.y\n",
    "\t\t\tloss = F.nll_loss(out, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\tloss_train += loss.item()\n",
    "\t\t\tout_log.append([F.softmax(out, dim=1), y])\n",
    "\t\tacc_train, f1_train, precision_train, recall_train, auc_train = eval_deep(out_log, train_loader)\n",
    "\t\t[acc_val, f1_val, _, recall_val, auc_val, loss_val] = compute_test(model, val_loader, args.device)\n",
    "\n",
    "\t\tif loss_val < best_loss_val:\n",
    "\t\t\tbest_loss_val = loss_val\n",
    "\t\t\tbest_metrics[\"acc\"] = acc_val\n",
    "\t\t\tbest_metrics[\"auc\"] = auc_val\n",
    "\t\t\tbest_metrics[\"F1\"] = f1_val\n",
    "\t\t\tbest_metrics[\"loss\"] = loss_val\n",
    "\t\n",
    "\treturn best_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'bert'\n",
    "train_size = 0.6\n",
    "val_size = 0.2\n",
    "batch_size = 128\n",
    "model = 'gcnfn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, \n",
    "                device='cuda:0',\n",
    "                batch_size=128,\n",
    "                train_size=0.6,\n",
    "                val_size=0.2,\n",
    "                lr=0.001,\n",
    "                weight_decay=0.01,\n",
    "                nhid=128,\n",
    "                epochs=60,\n",
    "                concat=False,\n",
    "                feature='spacy',\n",
    "                model='gcnfn') -> None:\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.nhid = nhid\n",
    "        self.epochs = epochs\n",
    "        self.concat = concat\n",
    "        self.feature = feature\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import to_undirected, add_self_loops\n",
    "from torch_sparse import coalesce\n",
    "from torch_geometric.io import read_txt_array\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\"\"\"\n",
    "\tFunctions to help load the graph data\n",
    "\"\"\"\n",
    "\n",
    "def read_file(folder, name, dtype=None):\n",
    "\tpath = os.path.join(folder, '{}.txt'.format(name))\n",
    "\treturn read_txt_array(path, sep=',', dtype=dtype)\n",
    "\n",
    "\n",
    "def split(data, batch):\n",
    "\t\"\"\"\n",
    "\tPyG util code to create graph batches\n",
    "\t\"\"\"\n",
    "\n",
    "\tnode_slice = torch.cumsum(torch.from_numpy(np.bincount(batch)), 0)\n",
    "\tnode_slice = torch.cat([torch.tensor([0]), node_slice])\n",
    "\n",
    "\trow, _ = data.edge_index\n",
    "\tedge_slice = torch.cumsum(torch.from_numpy(np.bincount(batch[row])), 0)\n",
    "\tedge_slice = torch.cat([torch.tensor([0]), edge_slice])\n",
    "\n",
    "\t# Edge indices should start at zero for every graph.\n",
    "\tdata.edge_index -= node_slice[batch[row]].unsqueeze(0)\n",
    "\tdata.__num_nodes__ = torch.bincount(batch).tolist()\n",
    "\n",
    "\tslices = {'edge_index': edge_slice}\n",
    "\tif data.x is not None:\n",
    "\t\tslices['x'] = node_slice\n",
    "\tif data.edge_attr is not None:\n",
    "\t\tslices['edge_attr'] = edge_slice\n",
    "\tif data.y is not None:\n",
    "\t\tif data.y.size(0) == batch.size(0):\n",
    "\t\t\tslices['y'] = node_slice\n",
    "\t\telse:\n",
    "\t\t\tslices['y'] = torch.arange(0, batch[-1] + 2, dtype=torch.long)\n",
    "\n",
    "\treturn data, slices\n",
    "\n",
    "\n",
    "def read_graph_data(folder, feature):\n",
    "\t\"\"\"\n",
    "\tPyG util code to create PyG data instance from raw graph data\n",
    "\t\"\"\"\n",
    "\n",
    "\tnode_attributes = sp.load_npz(folder + f'new_{feature}_feature.npz')\n",
    "\tedge_index = read_file(folder, 'A', torch.long).t()\n",
    "\tnode_graph_id = np.load(folder + 'node_graph_id.npy')\n",
    "\tgraph_labels = np.load(folder + 'graph_labels.npy')\n",
    "\n",
    "\n",
    "\tedge_attr = None\n",
    "\tx = torch.from_numpy(node_attributes.todense()).to(torch.float)\n",
    "\tnode_graph_id = torch.from_numpy(node_graph_id).to(torch.long)\n",
    "\ty = torch.from_numpy(graph_labels).to(torch.long)\n",
    "\t_, y = y.unique(sorted=True, return_inverse=True)\n",
    "\n",
    "\tnum_nodes = edge_index.max().item() + 1 if x is None else x.size(0)\n",
    "\tedge_index, edge_attr = add_self_loops(edge_index, edge_attr)\n",
    "\tedge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes, num_nodes)\n",
    "\n",
    "\tdata = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\tdata, slices = split(data, node_graph_id)\n",
    "\n",
    "\treturn data, slices\n",
    "\n",
    "\n",
    "class ToUndirected:\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tPyG util code to transform the graph to the undirected graph\n",
    "\t\t\"\"\"\n",
    "\t\tpass\n",
    "\n",
    "\tdef __call__(self, data):\n",
    "\t\tedge_attr = None\n",
    "\t\tedge_index = to_undirected(data.edge_index, data.x.size(0))\n",
    "\t\tnum_nodes = edge_index.max().item() + 1 if data.x is None else data.x.size(0)\n",
    "\t\t# edge_index, edge_attr = add_self_loops(edge_index, edge_attr)\n",
    "\t\tedge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes, num_nodes)\n",
    "\t\tdata.edge_index = edge_index\n",
    "\t\tdata.edge_attr = edge_attr\n",
    "\t\treturn data\n",
    "\n",
    "\n",
    "\n",
    "class FNNDataset(InMemoryDataset):\n",
    "\t\"\"\"\n",
    "\t\tThe Graph datasets built upon FakeNewsNet data\n",
    "\n",
    "\tArgs:\n",
    "\t\troot (string): Root directory where the dataset should be saved.\n",
    "\t\tname (string): The `name\n",
    "\t\t\t<https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n",
    "\t\t\tdataset.\n",
    "\t\ttransform (callable, optional): A function/transform that takes in an\n",
    "\t\t\t:obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "\t\t\tversion. The data object will be transformed before every access.\n",
    "\t\t\t(default: :obj:`None`)\n",
    "\t\tpre_transform (callable, optional): A function/transform that takes in\n",
    "\t\t\tan :obj:`torch_geometric.data.Data` object and returns a\n",
    "\t\t\ttransformed version. The data object will be transformed before\n",
    "\t\t\tbeing saved to disk. (default: :obj:`None`)\n",
    "\t\tpre_filter (callable, optional): A function that takes in an\n",
    "\t\t\t:obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "\t\t\tvalue, indicating whether the data object should be included in the\n",
    "\t\t\tfinal dataset. (default: :obj:`None`)\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, root, name, feature='spacy', empty=False, transform=None, pre_transform=None, pre_filter=None):\n",
    "\t\tself.name = name\n",
    "\t\tself.root = root\n",
    "\t\tself.feature = feature\n",
    "\t\tsuper(FNNDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "\t\tif not empty:\n",
    "\t\t\tself.data, self.slices, self.train_idx, self.val_idx, self.test_idx = torch.load(self.processed_paths[0])\n",
    "\n",
    "\t@property\n",
    "\tdef raw_dir(self):\n",
    "\t\tname = 'raw/'\n",
    "\t\treturn os.path.join(self.root, self.name, name)\n",
    "\n",
    "\t@property\n",
    "\tdef processed_dir(self):\n",
    "\t\tname = 'processed/'\n",
    "\t\treturn os.path.join(self.root, self.name, name)\n",
    "\n",
    "\t@property\n",
    "\tdef num_node_attributes(self):\n",
    "\t\tif self.data.x is None:\n",
    "\t\t\treturn 0\n",
    "\t\treturn self.data.x.size(1)\n",
    "\n",
    "\t@property\n",
    "\tdef raw_file_names(self):\n",
    "\t\tnames = ['node_graph_id', 'graph_labels']\n",
    "\t\treturn ['{}.npy'.format(name) for name in names]\n",
    "\n",
    "\t@property\n",
    "\tdef processed_file_names(self):\n",
    "\t\tif self.pre_filter is None:\n",
    "\t\t\treturn f'{self.name[:3]}_data_{self.feature}.pt'\n",
    "\t\telse:\n",
    "\t\t\treturn f'{self.name[:3]}_data_{self.feature}_prefiler.pt'\n",
    "\n",
    "\tdef download(self):\n",
    "\t\traise NotImplementedError('Must indicate valid location of raw data. No download allowed')\n",
    "\n",
    "\tdef process(self):\n",
    "\n",
    "\t\tself.data, self.slices = read_graph_data(self.raw_dir, self.feature)\n",
    "\n",
    "\t\tif self.pre_filter is not None:\n",
    "\t\t\tdata_list = [self.get(idx) for idx in range(len(self))]\n",
    "\t\t\tdata_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\t\t\tself.data, self.slices = self.collate(data_list)\n",
    "\n",
    "\t\tif self.pre_transform is not None:\n",
    "\t\t\tdata_list = [self.get(idx) for idx in range(len(self))]\n",
    "\t\t\tdata_list = [self.pre_transform(data) for data in data_list]\n",
    "\t\t\tself.data, self.slices = self.collate(data_list)\n",
    "\n",
    "\t\t# The fixed data split for benchmarking evaluation\n",
    "\t\t# train-val-test split is 20%-10%-70%\n",
    "\t\tself.train_idx = torch.from_numpy(np.load(self.raw_dir + 'train_idx.npy')).to(torch.long)\n",
    "\t\tself.val_idx = torch.from_numpy(np.load(self.raw_dir + 'val_idx.npy')).to(torch.long)\n",
    "\t\tself.test_idx = torch.from_numpy(np.load(self.raw_dir + 'test_idx.npy')).to(torch.long)\n",
    "\n",
    "\t\ttorch.save((self.data, self.slices, self.train_idx, self.val_idx, self.test_idx), self.processed_paths[0])\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn '{}({})'.format(self.name, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/anaconda3/envs/fake_news/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Arguments object at 0x7f454cf359d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:00<00:15,  3.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:11<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results\n",
      "+----+----------+----------+----------+----------+\n",
      "|    |      acc |      auc |     loss |       F1 |\n",
      "|----+----------+----------+----------+----------|\n",
      "|  0 | 0.822581 | 0.939103 | 0.325995 | 0.849315 |\n",
      "+----+----------+----------+----------+----------+\n",
      "Test Results\n",
      "+----+------------+---------+-------------+----------+----------+----------+\n",
      "|    |   Accuracy |      F1 |   Precision |   Recall |      AUC |     Loss |\n",
      "|----+------------+---------+-------------+----------+----------+----------|\n",
      "|  0 |   0.828125 | 0.84058 |    0.763158 | 0.935484 | 0.936461 | 0.445319 |\n",
      "+----+------------+---------+-------------+----------+----------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "\ttorch.cuda.manual_seed(SEED)\n",
    "\n",
    "dataset = FNNDataset(root='../data', feature=args.feature, empty=False, name='politifact', transform=ToUndirected())\n",
    "\n",
    "args.num_classes = dataset.num_classes\n",
    "args.num_features = dataset.num_features\n",
    "\n",
    "print(args)\n",
    "\n",
    "num_training = int(len(dataset) * args.train_size)\n",
    "num_val = int(len(dataset) * args.val_size)\n",
    "num_test = len(dataset) - (num_training + num_val)\n",
    "\n",
    "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if args.model == 'gcnfn':\n",
    "\tmodel = GCNFN(args=args)\n",
    "else:\n",
    "\tmodel = SimpleGNN(args)\n",
    "model = model.to(args.device)\n",
    "\n",
    "best_results = train(model, train_loader, val_loader, args)\n",
    "best_results_df = pd.DataFrame([best_results])\n",
    "print(\"Validation Results\")\n",
    "print(tabulate(best_results_df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "test_results = compute_test(model, test_loader, args.device, verbose=False)\n",
    "test_results_df = pd.DataFrame([test_results], columns=[\"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"AUC\", \"Loss\"])\n",
    "print(\"Test Results\")\n",
    "print(tabulate(test_results_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
